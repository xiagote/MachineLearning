https://zhuanlan.zhihu.com/p/35356992
　　
　　讲讲正则化为什么能降低过拟合程度,并且说明下L1正则化和L2正则化
  
  ---
  
#### L1和L2正则化:

　　我们所说的正则化,就是在原来的loss function的基础上,加上了一些正则化项或者称为模型复杂度惩罚项.现在我们还是以最熟悉的线性回归为例子.
　　
　　优化目标:
  $$min \frac{1}{N} \cdot \sum_{i=1}^N(y_i - w^Tx_i)^2 \tag{1}$$
  
　　加上$L_1$正则化(lasso回归):
  $$min \frac{1}{N} \cdot \sum_{i=1}^N(y_i - w^Tx_i)^2  + C||w||_1 \tag{2}$$
  
　　加上$L_2$正则化(岭回归):
  
$$min \frac{1}{N} \cdot \sum_{i=1}^N(y_i - w^Tx_i)^2  + C||w||_2^2 \tag{2}$$

#### 结构风险最小化角度:

　　结构风险最小化:在经验风险最小化的基础上(也就是训练误差最小化),尽可能采用简单的模型,以此提高模型泛化精度.
  
　　那么我们现在看看加了$L_1$正则化和$L_2$正则化之后,目标函数求解的时候,最终解有什么变化.
  
　　图像解释(假设X为一个二维样本,那么要求解参数$w$也是二维)
  
　　原函数曲线等高线(同颜色曲线上,每一组$w_1$,$w_2$代入值都相同)
  <div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/PCA/2d_projection.png"/></div>
  
　　加入$L_1$正则化和$L_2$正则化后的图像:
  <div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/PCA/2d_projection.png"/></div>
  
　　从上边两幅图中我们可以看出:

　　(1)如果不加$L_1$正则化和$L_2$正则化的时候,对于线性回归这种目标函数是凸函数而言,我们最终的结果就是最里面的紫色小圈圈等高线上的点.
  
　　(2)当加入$L_1$正则化和$L_2$正则化的时候,我们先画出$|w_1| + |w_2| = F$的图像,也就是一个菱形,代表这些曲线上的点算出来的1范数$|w_1| + |w_2|$都为F.那我们现在的目标是不仅是原曲线算的值要小(越小越接近中心的紫色圈圈),还要使得这个菱形越来越小(F越小越好).那么还和原来一样的化,过中心紫色圈圈的那个菱形明显很大,因此我们要取到一个恰好的值.那么如果求值呢?
 
 <div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/PCA/2d_projection.png"/></div>
　　
　　(3)以同一条原曲线目标等高线来说,现在以最外圈的红色等高线为例.我们看到,对于红色曲线上的每个点都可以做一个菱形,根据上图可知,当这个菱形与某条等高线相切(仅有一个交点)的时候,这个菱形最小,上图相割比较大的两个菱形对应的1范数更大.用公式说这个时候能使得在相同$1/N \cdot \sum_{i=1}^N(y_i - w^Tx_i)^2$下,由于相切的时候$C||w||_1$小,即$|w_1|+|w_2|$小,所以:能够使得$1/N \cdot \sum_{i=1}^N(y_i - w^Tx_i)^2 + C||w||_1$更小(这里可以考虑拉格朗日对偶问题)
  
　　(4)我们可以看出,最终加入$L_1$范数得到的解,一定是某个菱形和某条原函数等高线的切点.现在有个比较重要的结论来了:我们经过观察可以看到,几何对于很多原函数等高曲线,和某个菱形相交的时候及其容易相交在坐标轴(比如上图),也就是说最终的效果,解的某些维度及其容易是0,这就是我们所说的$L_1$更容易得到稀疏解(解向量中0比较多)的原因.
  
  ---
  
 #### 贝叶斯先验的角度:
 
https://blog.csdn.net/zhuxiaodong030/article/details/54408786

##### 贝叶斯理论

　　贝叶斯决策理论是主观贝叶斯派归纳的重要组成部分.贝叶斯决策就是在不完全情报下,对部分未知的状态用主观概率估计,然后用贝叶斯公式对发生概率进行修正,最后再利用期望值和修正概率做出最优决策.
  
　　贝叶斯决策理论方法是统计模型决策中的一个基本方法,其基本思想是:
　　1.已知类条件概率密度参数表达式和先验概率
　　2.利用贝叶斯公式转换后成后验概率
　　3.根据后验概率大小进行决策分类
  
　　设$D_1,D_2,\cdots,D_n$为样本空间S的一个划分,如果以$P(D_i)$表示事件$D_i$发生的概率,且$P(D_i)>0\ (i=1,2,\cdots,n)$
  
　　对于任一事件x,P(x)>0,如下式
  
$$P(D_j/x) = \frac{p(x/D_j)P(D_j)}{\sum_{i=1}^nP(X/D_i)P(D_i)}$$

##### 从贝叶斯角度理解正则化

　　从我们平时最为熟悉的最小二乘回归、Ridge回归和LASSO回归入手.
　　
　　从概率论的角度:

　　　　Least Square 的解析解可以用Gaussian分布以及最大似然估计求得

　　　　Ridge回归可以用Gaussian分布和最大后验估计解释

　　　　LASSO回归可以用Laplace分布和最大后验估计解释

　　给定观察数据D,贝叶斯方法通过最大后验概率估计参数w.
  
$$w^{\ast} = arg\max x_wp(w|D)=arg \max x_w \frac{p(D|w)\ast p(w)}{p(D)}=arg \max x_w p(D|w)p(w)$$

　　其中:$p(D|w)$是似然函数(likelihood function):参数向量w的情况下,观测数据D出现的概率;$p(w)$是参数向量的先验概率(prior)
  
　　对于似然函数部分有:
  
  $$p(D|w) = \prod_{k=0}^n p(D_i|w)$$
　　
　　则,对后验概率取对数有:
$$
  \begin{array}{}
  \quad w_{\ast}& =& arg \max x_w p(D|w)p(w)\\
  \\
  &=& arg \max x_w \prod_{k=0}^n p(D_i|w)p(w)\\
  \\
  &=& arg \max x_w \sum_{k=0}^n \log p(D|w) + \log p(w) \\
  \\
  &=& arg \min _w - \sum_{k=0}^n \log p(D|w) - \log p(w)
  \end{array}
$$

　　当先验概率分布满足正态分布的时候
　　$$p(w_i) = N(w_i|\mu,\theta ^2) = \frac{1}{\sqrt{2\pi \theta ^2}}e^{-\frac{(w_i-\mu)^2}{2\theta ^2}}$$
  
　　代入式子展开可以得到
  
$$
  \begin{array}{}
  \quad w_{\ast}& =& arg \min - \log \sum_{k=0}^np(D_i|w) - \log p(w)\\
  \\
  &=& arg \min _w -\log\sum_{k=0}^np(D_i|w) - \sum_{i=0}^m \log p(w_i)\\
  \\
  &=& arg \min _w -\log\sum_{k=0}^np(D_i|w) + \sum_{i=0}^m \frac{1}{\theta^2}(w_i - \mu)^2 \\
  \\
  &=& arg \min _w - \sum_{k=0}^n \log p(D|w) + \lambda\sum_{i=0}^m(w_i)^2
  \end{array}
$$

　　对比下式
$$w^{\ast} = arg \min\sum_{i}L(y_i,f(x_i;w))+\lambda \Omega(w)$$

　　可以看到,似然函数部分对应于损失函数(经验风险),而先验概率部分对应于正则化.L2正则,等价于参数w的先验概率分布满足正态分布.
  
　　当先验概率分布满足拉普拉斯分布的时候
  
  $$p(w_i)=N(w_i|\mu,b)=\frac{1}{2b}e^{\frac{|w_i-\mu|}{b}}$$

　　可以得到
