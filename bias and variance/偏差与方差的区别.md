https://blog.csdn.net/hertzcat/article/details/80035330 (吴恩达机器学习:方差与偏差)

#### 数据集划分

　　为了评价模型,我们通常将数据集分为三个部分,60%的训练集、20%的交叉验证集和20%的测试集,并使用误差作为模型使用在这些集合上的评价.评价的形式与之前的代价函数相同.(线性回归误差函数如下):

$$J_s(\theta) = \frac{1}{2m_s} \sum_{i=1}^{m_s}(h_{\theta}(x_s^{(i)}) - y_s(i))^2\ \ \ (s=train,cv,test)$$

　　在被划分的集合中,我们使用训练集来参数$\theta$,使用交叉验证集来选择模型(比如该使用多少次多项式特征),使用测试集来评估模型的预测能力.

#### 方差与偏差

　　当我们的模型表现不佳时,通常是出现两种问题,一种是 <font color= '#dd0000'>高偏差</font> 问题,另一种是  <font color= '#dd0000'>高方差</font> 问题.识别它们有助于选择正确的优化方法,所以我们先看下<font color= '#dd0000'>偏差</font> 与<font color= '#dd0000'>方差</font> 的意义

　　-偏差:描述模型输出结果的期望与样本真实结果的差距

　　-方差:描述模型对于给定值的输出稳定性.

<div align=center><img width="400" height="200" src="https://github.com/xiagote/MachineLearning/blob/master/bias%20and%20variance/bias_and_variance.png"/></div>

　　就像打靶一样,偏差描述了我们的射击总体是否偏离了我们的目标,而方差描述了射击准不准.接下来我们通过各种情况下<font color= '#dd0000'>训练集</font> 与<font color= '#dd0000'>交叉验证集</font>的误差曲线来直观地理解 <font color= '#dd0000'>偏差</font> 与<font color= '#dd0000'>方差</font> 的意义

　　对于多项式回归,当次数选取较低时,我们的训练集误差和交叉验证集误差都会很大;当次数选择刚刚好时,训练误差和交叉验证误差都很小;但当次数过大时会产生过拟合,虽然训练误差很小,但交叉验证误差会很大(关系图如下)

<div align=center><img width="400" height="200" src="https://github.com/xiagote/MachineLearning/blob/master/bias%20and%20variance/err_polynomial.png"/></div>

　　所以我们可以计算$J_{train}(\theta)$和$J_{cv}(\theta)$,如果他们同时很大的化,就是遇到了高偏差问题;

<div align=center><img width="400" height="200" src="https://github.com/xiagote/MachineLearning/blob/master/bias%20and%20variance/err_polynomial_2.png"/></div>

　　对于正则化参数,使用同样的分析方法,当参数比较小时容易产生过拟合现象,也就是高方差问题.而参数比较大时容易产生欠拟合现象,就是高偏差.

#### 学习曲线

　　无论你是要检查学习算法是否正常工作或需要改进算法的算法的表现,学习曲线都是一个十分直观有效的工具.学习曲线的横轴是样本数,纵轴为训练集和交叉验证集的误差.所以在一开始,由于样本数少,$J_{train}(\theta)$几乎没有,而$J_{cv}(\theta)$则非常大.随着样本数的增加,$J_{train}(\theta)$不断增大,而$J_{cv}(\theta)$因为训练数据增加而拟合得更好因此下降.所以学习曲线如下图:


<div align=center><img width="400" height="200" src="https://github.com/xiagote/MachineLearning/blob/master/bias%20and%20variance/learning_curve1.png"/></div>

　　在高偏差的情形下,$J_{train}(\theta)$与$J_{cv}(\theta)$已经十分接近,但是误差看大.这时候一味地增加样本数并不能给算法的性能带来提升.

<div align=center><img width="400" height="200" src="https://github.com/xiagote/MachineLearning/blob/master/bias%20and%20variance/learning_curve2.png"/></div>

　　在高方差的情形下,$J_{train}(\theta)$的误差较小,$J_{cv}(\theta)$比较大,这时搜集更多的样本很可能带来帮助.

<div align=center><img width="400" height="200" src="https://github.com/xiagote/MachineLearning/blob/master/bias%20and%20variance/learning_curve3.png"/></div>

---

https://www.zhihu.com/question/20448464

#### 引子

　　假设我们有一个回归问题,我们搞到一批训练数据D,然后选择了一个模型M,并用数据D将M训练出来,记作$M_t$,这里我们故意把模型M与训练出的模型$M_t$区分开,是为了后面叙述时概念上的清晰.

　　我们都知道,模型M代表的是一个函数空间,比如模型$y=wx+b$,若x,y都是实数,w,b为实数参数,则该模型就代表了平面上所有的直线,这所有的直线就是一个函数空间.

　　同理,$y=ax^2+bx+c$代表的就是平面上所有的二次曲线,所有的二次曲线组成一个函数空间.当然,所有的直线此时也是二次曲线的特例.

　　回到上面的问题,$M_t$实际上使用数据D找到的M代表的函数空间中的一个具体的函数.$M_t$的表现好坏不能完整地代表M的好坏.

#### 什么是M的好坏

　　以上面的一次函数和二次函数为例,当我们说二次函数比一次函数更好时,我们潜在的含义是说,对于某个我们正要解决的机器学习问题来说,二次函数总体上比一次函数表现更好,我们是在函数空间的层次上来比较的.

　　而且,还是针对一个具体的机器学习问题来比较的,因为对于不同的机器学习问题,二者哪个更好是不一定的.

　　Note:在下文中,我们把机器学习问题默想成回归问题,这样便于理解.

　　这里再次强调,当我们说模型好坏时,隐含有两个含义:

　　　　1、比较的是整个函数空间

　　　　2、针对某个具体机器学习问题比较


#### 怎么比较M的好坏

　　我们可以这样做

　　　1)找一条不变的万能测试样本

　　　　在这个具体的机器学习问题中找一条样本x,它的标签为y.在后续的所有训练中都用这条样本做测试集,永远不用作训练集.

　　　2)在测试样本上观察$M_t$的表现,假设$M_t$在样本x上的预测为$y_t$,则$y-y_t$可用来评价$M_t$的表现好坏.

　　　3)找另外一个训练集$D_1$,训练出$M_{t1}$,在测试样本上测试得到$y_{t1}$,进而得到误差$y-y_{t1}$

　　　4)重复第3步多次,知道得到N个具体的模型,和N个$y_t$,N个$y-y_t$

　　　5)当N足够大时,我们可以这样来评测M的好坏,首先看N个$y_t$的均值$y_{tmean}$是否等于y,其次,看N个$y_t$相对均值$y_{tmean}$的方差有多大.


　　显然,若$y_{tmean}=y$,说明M的学习能力是够的,也就是说,当N趋向无穷大是,N个$M_t$对x预测的均值能无限接近y.

　　很多人会有种错觉,感觉任何M都能达到上面的效果,实际上,不是每一个M都有这样的能力的,举个极端的例子,我们假设$M_1$的函数空间中只有一个函数,且对于任何样本的预测值都恒等于y+1,则无论N多大,$y_{tmean}$都
会比y大1.我们称$M_1$由于学习能力不够造成的对x的预测误差叫做偏差.

　　其次,N个$y_t$相对均值$y_{tmean}$的方差有多大也能从另一个方面揭示M的好坏,举个例子,假设我们有$M_1$,$M_2$两个模型,当N无穷大时,都能使得$y_{tmean}$等于y.但是M的预测值是这样分布的(下面圆点代表一个个的预测值)

$$.....y_{tmean}....$$

$M_2$的预测值是这样分布的

$$.\ .\ .\ y_{tmean}\ .\ .\ $$
　　显然,我们会觉得$M_1$比$M_2$更好.你可能回想,N足够大时,二者都能准确地得到均值y,这就够了,没必要再比较它们的预测值相对均值的方差.

　　这样的观点的错误的地方是:时间中,我们并不能抽样出$D_1,D_2,D_3,...,D_N$个训练集,往往只有一份训练集D,这种情况下,显然,用$M_1$比用$M_2$更有把握得到更小的误差.

#### 举个例子来说明偏差方差

　　假设模型是一个射击学习者,D1,D2直到DN就是N个独立的训练计划.

　　如果一个学习者是正常人,一个眼睛斜视,则可以想见,斜视者无论参加多少训练计划,都不会打中靶心,问题不在训练计划够不够好,而在他的先天缺陷.这就是模型偏差产生的原因,学习能力不够.正常人参加N个训练计划后,虽然也不能保证打中靶心,但随着N的增大,会越来越接近靶心.

　　假设还有一个超级学习者,他的学习能力特别强,参加训练计划D1时,他不仅学会了瞄准靶心,还敏感地捕捉到了训练时的风速,光线,并据此调整了瞄准的方向,此时,他的训练成绩会很好.

　　但是,当参加测试时的光线,风速肯定与他训练时是不一样的,他仍然按照训练时学来的瞄准方法去打靶,肯定是打不好。这样产生的误差就是方差。这叫做聪明反被聪明误.

　　总结一下:学习能力不行造成的误差是偏差,学习能力太强造成的误差是方差.

