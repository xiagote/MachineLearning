文档来源于： http://blog.codinglabs.org/articles/pca-tutorial.html

　　PCA(Prinicpal Component Analysis) 是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征变量，常用于高纬数据的降维。这篇文章的目的是介绍PCA的基本数学原理。

### 数据的向量表示及降维问题

　　一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量计交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下。
    
　（日期，浏览量，访客数，下单数，成交数，成交金额）
 
　　其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子：
  
$$(500,240,25,13,2312.15）^T$$

　　注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。
  
　　我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有密切关系，甚至与维数呈指数级关系。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行处理，去除不重要的特征，即降维。

　　降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。
  
　　举个例子，假设某学籍数据有两列M和F，其中M列的取值是学生的性别，男性取值为1，女性取值为0；而F列是女性取值为1，男性取值为0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1.在这种情况下，我们将M或F去掉不会造成任何信息的损失，因为只要保留一列就能完全还原另一列。

　　上面一种情况是一个极端的情况，在现实中基本不能这么完美，但是通常有类似的情况。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词。
  
　　这种情况表明，如果我们删除浏览量或访客数其中一个指标，应该能猜想到并不会丢失太多信息。因此可以删除一个，减少特征，降低机器学习的复杂度。
  
### 向量的表示级基变换

　　既然我们面对的数据被抽象维一组向量，那么我们有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。
  
#### 内积与投影

　　两个维数相同的向量的内积被定义为：
  
  $$（a_1,a_2,\cdots,a_n）^T \cdot (b_1,b_2,\cdots,b_n)^T\ =\ a_1b_1 + a_2b_2 + \cdots + a_nb_n$$
  
　　内积运算将两个变量映射为一个实数。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则$A\ =\ （x_1,y_1）$,$B\ =\ (x_2,y_2)$。则在二维平面上A和B可以用两条发自原点的有向线段表示。如下图
  
  <div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/PCA/2d_projection.png"/></div>

　　好,我们现在从A点向B点所在直线引一条垂线.我们知道垂线与B的交点叫做A在B上的投影,再设A与B的夹角是$\alpha$,则投影的矢量长度为$|A|\cos(\alpha)$,其中$|A|=\sqrt(x_1^2+y_1^2)$是向量A的模,也就是A线段的标量长度.

　　注意这里我们专门区分了矢量长度和标量长度,标量长度总是大于等于0,值是线段的长度;而矢量长度可能为负,其绝对值是线段长度,而符号取决于其方向与标准方向相同或相反.
  
#### 基
  
　　下面我们继续在二维空间内讨论向量.上文说过,一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段.例如下图是这个向量:
  

  <div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/PCA/2dvector.png"/></div>

　　在代数表示方面,我们经常用线段终点的点坐标表示向量,例如上面的向量可以表示为(3,2),这是我们再熟悉不过的向量表示.
  
　　不过我们常常忽略,只有一个(3,2)本身是不能够精确表示一个向量的.我们仔细看一下,这里的3实际表示的是向量在x轴上的投影值是3,在y轴上的投影是2.也就是说我们其实隐式引入了一个定义:以x轴和y轴上正方向长度为1的向量为标准.那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2.注意投影是一个矢量,所以可以为负.
  
　　更正式的说,向量(x,y)实际上表示线性组合:
  $$x(1,0)^T+y(0,1)^T$$
  
　　不难证明所有二维向量都可以表示为这样的线性组合.此处(1,0)和(0,1)叫做二维空间中的一组基.
  
  
