https://www.zhihu.com/question/37031188

　　一个向量在另一个向量上的投影,实际上就是寻找在线上最近的点.
  
<div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/tree/master/least%20square%20method/projection.png></div>

　　现在我们假设投影点是向量上的一点p,可以规定$\vec{p}=x\vec{a}$(x是某个数).定义$\vec{e} = \vec{b} - \vec{p}$,称$\vec{e}$为误差.因为$\vec{e}$与$\vec{p}$也就是$\vec{a}$垂直,所以$\vec{a}^T(\vec{b} - \vec{a})x = 0$,展开化简得到:
  
  $$x = \frac{\vec{a}^T}{\vec{a}^T\vec{a}} \cdot \vec{b}\ ,\ \vec{p} = \vec{a}x = \frac{\vec{a} \vec{a}^T}{\vec{a}^T \vec{a}} \cdot \vec{b}$$
  
　　我们发现:如果改变$\vec{b}$,那么$\vec{p}$相对应改变,然而改变$\vec{a}$,p无变化.接下来,我们可以考虑更高维度的投影,三维空间的投影是怎样的呢,我们可以想想一个三维空间内的向量在该空间内的一个平面上的投影:
  
<div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/least%20square%20method/projection_2.png/></div>

　　我们假设这个平面的基(basis)是$a_1$和$a_2$.那么矩阵A的列空间就是该平面.假设一个不在该平面上的向量$\vec{b}$在该平面上的投影是$\vec{p}$.我们的任务就是就是找到合适的x,使得$\vec{p}=A\vec{x}$,这里有一个关键的地方:e与该平面垂直,所以$A^T(\vec{b} - A\vec{x}) = 0$.我们把上边式子展开,得到
  
  $$x = (A^TA)^{-1}A^T \cdot \vec{b}\ ,\ p = A\vec{x} = A(A^TA)^{-1}A^T \cdot \vec{b}\ $$
  
　　有了上面的背景知识,我们可以正式进入主题了,投影矩阵(projection matrix):
  
  $$P=A(A^TA)^{-1}A^T$$
  
　　这里我们最需要关注的是投影矩阵的两个性质:

　　(1)P^T=P

　　(2)P^2=P
  
　　对于第一个,很容易理解,因为P本身就是个对称阵.第二个,直观的理解就是投影到a上后再投影一次,显然投影并没有改变,也就是二次投影还是其本身.
  
---

　　这个投影到底有什么用呢?从上面的分析中我们可以看出:投影矩阵P可以把向量b投影成向量p!从线性代数的角度来说,$\vec{b}=A\vec{x}$并不一定总有解,这在实际情况中会经常遇到$(m>n)$.所以我们就把b投影到向量p上,因为p在$\vec{a_1},\vec{a_2}$的平面内,所以$\vec{p}=A\vec{x}$是可以求解的.
  
---

　　好了,我们先暂别"投影".下面,开始说一下最小二乘的故事吧:在实际应用中,线性回归是经常用到的,我们可以在一张散列点图中做一条直线(暂时用直线)来近视表述这些散列点的关系.比如:
  
  <div align=center><img width="300" height="300" src="https://github.com/xiagote/MachineLearning/blob/master/least%20square%20method/least_square.png"/></div>
  
　　设变量y与t成线性关系,即现在已知m个实验点$a_i$和$b_i$,求两个未知参数C,D.将其代入得到矛盾方程组
  
<div align=center><img width="300" height="700" src="https://github.com/xiagote/MachineLearning/blob/master/least%20square%20method/formula.png></div>

　　从线性代数的角度来看,就是A的列向量的线性组合无法充满整个列空间,也就是说$\vec{b} = A\vec{x}$这个方程根本没有解.从图形上也很好理解:根本没有一条直线同时经过所有蓝色的点!所以为了选取最合适的x,让该等式"尽量成立",引入残差平方和函数H:
  
  $$\min(H) = \min(||e||^2) = \min(||b - Ax||)^2$$
  
　　这也就是最小二乘法的思想.我们知道,当x取最优值的时候,Ax恰好对应图中线上橙色的点,而b则对应图中蓝色的点,e的值则对应红色的线长.
  
  ---
  
　　看到这里你有没有和之前投影的那部分知识联系在一起呢?最小二乘的思想是如何选取参数x使得H最小.从向量投影的角度来看这个问题,H就是向量e的平方,如何才能使e的长度最小呢?b和$a_1,a_2$都是固定的,当然是e垂直$a_1,a_2$平面的时候长度最小.换句话说:最小二乘法的解与矩阵投影是对变量求解的目标是一致的.
  
　　于是,根据矩阵投影的知识,我们可以直接写出最小二乘法问题的解
  
$$
x=
\left(
\begin{matrix}
C \\
D
\right)
\end{matrix}=
(A^TA)^{-1}A^Tb
$$
　　其中A称为结构矩阵,b称为数据矩阵,$A^TA$称为信息矩阵,$A^Tb$称为常数矩阵.而最小二乘的求解实质上就是$\vec{b}=A\vec{x}$没有解,我们就把b投影到向量p上,求解$\vec{p}=A\vec{x}$
  
---

　　写到这里,我想有必要总结一下,为什么最小二乘法和投影矩阵要扯到一起,它们有什么联系:最小二乘是用于数据拟合的一个很霸气的方法,这个拟合过程我们称之为线性回归.如果数据点步存在离群点(outliers),那么该方法总是会显示其简单粗暴的一面.我们可以把最小二乘的过程用矩阵的形式描述出来,然而,精妙之处就在于,这与我们的投影矩阵的故事不谋而合,所以,我们又可以借助投影矩阵的公式,也就是$A^TAx=A^Tb$来加以解决
